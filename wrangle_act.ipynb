{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Project: Wrangling and Analyze Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import tweepy\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Directly download the WeRateDogs Twitter archive data (twitter_archive_enhanced.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading file into dataframe\n",
    "twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the Requests library to download the tweet image prediction (image_predictions.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieving image prediction file\n",
    "image_predictions_url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "\n",
    "with open('image-predictions.tsv', 'wb') as image_predictions_file:\n",
    "    gathered = requests.get(image_predictions_url)\n",
    "    image_predictions_file.write(gathered.content)\n",
    "  \n",
    "# Reading file into dataframe\n",
    "tweet_predictions = pd.read_csv('image-predictions.tsv', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the Tweepy library to query additional data via the Twitter API (tweet_json.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The below `tweepy.Client` instance references `credentials.py` to store the API connection token. I have not included this file in my submission for security reasons. In order for the code to function, a new `credentials.py` must be created with the following variable: <br>*\n",
    "\n",
    "**bearer_token** = (bearer token here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing connection token\n",
    "import credentials\n",
    "\n",
    "# Creating tweepy.Client instance \n",
    "client = tweepy.Client(bearer_token=credentials.bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing fields for tweet_json write process\n",
    "## start_time and count are to track the iteration of my for loops\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "\n",
    "# When using client, you have to grab the fields that you want to query\n",
    "tweet_fields = [\"public_metrics\"]\n",
    "\n",
    "# Initializing tweet_id list to store our tweet_ids for the successful queries\n",
    "tweet_id = []\n",
    "\n",
    "# Opening tweet_json with write privileges. \n",
    "## Iterating over each tweet_id from twitter_archive to find additional metrics via API call\n",
    "### Each successful query is written to the txt file and the tweet_id is stored in tweet_id list\n",
    "with open('tweet_json.txt', 'w') as file:\n",
    "    for single_id in twitter_archive['tweet_id']:\n",
    "        try:\n",
    "            temp = client.get_tweet(id = single_id, tweet_fields = tweet_fields)\n",
    "            tweet_data = temp.data\n",
    "            for key in tweet_fields:\n",
    "                if tweet_data.get(key):\n",
    "                    file.write(json.dumps(tweet_data[key]))\n",
    "                else:\n",
    "                    pass\n",
    "            file.write('\\n')\n",
    "            tweet_id.append(single_id)\n",
    "            count = count + 1\n",
    "            print(count)\n",
    "        except Exception as e:\n",
    "            print('No tweet found for {} with error message {}'.format(str(single_id), str(e)))\n",
    "\n",
    "# Once loop finishes, a statement containing total run time is printed\n",
    "end_time = time.time()\n",
    "print('Process finished in {} seconds'.format(start_time-end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing empty lines list to capture the data from our tweet_json.txt file\n",
    "lines = []\n",
    "\n",
    "# Iterating over each line in the list, extracting 'retweet_count' and 'like_count', storing to lines\n",
    "with open('tweet_json.txt', 'r') as txt:\n",
    "    for i in txt:\n",
    "        try:\n",
    "            row = json.loads(i)                \n",
    "            lines.append({\"retweet_count\":row['retweet_count'],\n",
    "                         \"favorite_count\":row['like_count']})\n",
    "        except Exception as e:\n",
    "            print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating columns for data frame\n",
    "columns = ['tweet_id', 'retweet_count', 'favorite_count']\n",
    "\n",
    "# Dataframe creation with lines and tweet_id data.  \n",
    "tweet_metrics = pd.DataFrame(lines, columns = columns)\n",
    "tweet_metrics['tweet_id'] = tweet_id\n",
    "\n",
    "# Displays first 2 lines of frame for validity check\n",
    "tweet_metrics.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving tweet_metrics to .csv\n",
    "## If I reload my notebook, I will lose the frame, which would require me to make the API calls again\n",
    "### I still have 'tweet_json', however I would lose the list of successfully queried tweet id's\n",
    "\n",
    "tweet_metrics.to_csv(\"tweet_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the gathering process we have the following data frames:\n",
    "\n",
    "`twitter_archive`: Contains the main data for the **We Rate Dogs** tweets <br>\n",
    "`tweet_predictions`: Contains prediction results from the neural network sampled off of **We Rate Dogs** <br>\n",
    "`tweet_metrics`: Contains additional public metrics (retweet_count, favorite_count) for tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_metrics = pd.read_csv('tweet_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 28,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I will display some basic info about each data frame for analysis. I will attempt to find any obvious quality or tidiness issues. After I identify any potential issues, I will explore further using programmatic analysis to help define the issue scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### twitter_archive Data Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display twitter_archive dataframe for visual analysis\n",
    "twitter_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the .info for twitter_archive\n",
    "twitter_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. href tags in `source`\n",
    "2. null data (NaN) appearing as string 'None' in several columns\n",
    "3. data type may need updating for timestamp fields\n",
    "4. data type for `tweet_id` to object??\n",
    "4. combine dog type columns?\n",
    "5. non-names in `name`\n",
    "6. only need original tweets. can remove replies and retweets\n",
    "7. `expanded_urls` missing values\n",
    "8. `rating_denominator` per schema should be /10. we have a min value of 0\n",
    "9. `rating_numerator` has a **large** max value of 1776. is this legit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `source` HTML Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Counting unique values\n",
    "twitter_archive.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: There are only 4 values in this column, which can be simplified to: \n",
    "1. Twitter for iPhone\n",
    "2. Vine - Make a Scene\n",
    "3. Twitter Web Client\n",
    "4. TweetDeck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `name` Value Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Attempting to determine 'bad names' by looking at unique values in name column\n",
    "twitter_archive.name.value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Appears that proper names are capitalized\n",
    "## Using regex to create a mask that will identify all strings in 'name' that start with a lower case letter\n",
    "lower_mask = twitter_archive.name.str.contains('^[a-z]', regex = True)\n",
    "twitter_archive[lower_mask].name.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking total count of lower case strings\n",
    "len(twitter_archive[lower_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displaying with more concise columns for review\n",
    "column_list = ['tweet_id', 'text', 'name']\n",
    "twitter_archive[lower_mask][column_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: Based on looking at the `text` fields, the incorrect `name` appears to come after 'is' in each example. The dog names do not appear in the tweet text, so we cannot just update the field with the correct name. Best solution is probably to set the invalid `name` values to `null`\n",
    "\n",
    "Ex: `tweet_id` = 887517139158093824 <br>\n",
    "`text` = 'I've yet to rate a Venezuelan Hover Wiener. This is such an honor. 14/10 paw-inspiring af (IG: roxy.thedoxy)'<br>\n",
    "`name` = 'such'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `expanded_urls` Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display total number of entries missing a URL\n",
    "sum(twitter_archive.expanded_urls.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can drop any rows that are replies or retweets, our actual count of null `expanded_urls` may be less than this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mask to identify rows where `expanded_urls`, `in_reply_to_status_id`, and `retweeted_status_id` are all null\n",
    "url_mask = (twitter_archive.expanded_urls.isna()) & (twitter_archive.in_reply_to_status_id.isna()) & (twitter_archive.retweeted_status_id.isna())\n",
    "\n",
    "# Getting the length of the mask results\n",
    "len(twitter_archive[url_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display records from url_mask\n",
    "twitter_archive[url_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: I checked these tweets manually and confirmed they are not valid ratings, nor do any of them have images. I'm going to drop these for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rating_denominator` Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the counts of all denominator values to examine\n",
    "twitter_archive.rating_denominator.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. We only have 1 tweet with a 0 value in the denominator. \n",
    "2. The majority of our tweets have '10' as the denominator. Most of the tweets without /10 appear to be multiples of 10. Should they actually be /10? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking more closely at 0 denominator tweet\n",
    "zero_mask = twitter_archive.rating_denominator == 0\n",
    "twitter_archive[zero_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the `in_reply_to_status_id` being non-null, this tweet will be dropped from our data set when we clean anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at tweets with non-10 values as the denominator \n",
    "## Excluding replies and retweets\n",
    "denom_mask = (twitter_archive.rating_denominator != 10) & \\\n",
    "            (twitter_archive.in_reply_to_status_id.isna()) & \\\n",
    "            (twitter_archive.retweeted_status_id.isna())\n",
    "                \n",
    "twitter_archive[denom_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of list with != 10 denominators\n",
    "len(twitter_archive[denom_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "There are 17 tweets that contain a number other than 10 as the denominator (that are not replies or retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking more closely at fields that may give us an explanation\n",
    "column_list = ['tweet_id', 'text', 'rating_numerator', 'rating_denominator' ]\n",
    "twitter_archive[denom_mask][column_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Some of these denominators look like they are pulled from the wrong part of `text`\n",
    "\n",
    "Ex:<br>\n",
    "`tweet_id` = `722974582966214656` <br>\n",
    "`text` = `Happy 4/20 from the squad! 13/10 for all`\n",
    "\n",
    "The numerator and denominator are 4/20, however from the text we can clearly see that the rating should be 13/10. There are only a few of these, so we can manually update the numerator and denomenator columns with the correct values. \n",
    "\n",
    "Additionally, some of these tweets appear to have multiple dogs included (ie: puppers)\n",
    "\n",
    "I'm not immediately sure how to handle instances of multiple dogs in an image. Since we are combining this frame with image breed predictions, it might make sense to just drop tweets with multiple dogs for our analysis. We have a small number of these compared to total tweets, it shouldn't impact our analysis too much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rating_numerator` Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display numerator counts\n",
    "twitter_archive.rating_numerator.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. Look at 0 values\n",
    "2. Look at large values >= 15? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a mask to view numerators == 0\n",
    "zero_mask = twitter_archive.rating_numerator == 0 \n",
    "\n",
    "twitter_archive[zero_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "One of these tweets will be dropped because its a reply. The other tweet appears to be legitimate. Per the tweet text, that is the correct rating. I will leave as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating mask to view tweets with a `large` numerator >= 15 and denominator == 10\n",
    "## Excluding retweets and replies\n",
    "large_mask = (twitter_archive.rating_numerator >=15) & \\\n",
    "            (twitter_archive.rating_denominator == 10) & \\\n",
    "            (twitter_archive.in_reply_to_status_id.isna()) & \\\n",
    "            (twitter_archive.retweeted_status_id.isna())\n",
    "twitter_archive[large_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of 'large_mask'\n",
    "len(twitter_archive[large_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive[large_mask][column_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: I chose 15 as my starting range since the majority of tweets have a numerator <= 15. Based on the tweet `text`, these values are either incorrectly converted decimals, or outliers. Since there are so few, I'm just going to drop them for analysis purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### twitter_archive  notes:\n",
    "\n",
    "Quality: \n",
    "1. source column contains unnecessary `<a href> </a>` tags \n",
    "2. `expanded_urls` contains 3 *bad* tweets\n",
    "2. NaN data appearing as string `None` in several columns: name, doggo, floofer, pupper, puppo\n",
    "3. `timestamp` and `retweeted_status_timestamp` should not be object, probably datetime\n",
    "4. name column contains 109 non-names ex: `a`\n",
    "5. 181 values exist in `retweeted*` fields. These should be removed\n",
    "6. 78 replies exist in `in_reply_to_status_id` and `in_reply_to_user_id` fields. These should be removed\n",
    "7. `tweet_id` should be 'object'? \n",
    "8. Invalid denominators should be manually corrected, or dropped\n",
    "9. Invalid numerators should be dropped \n",
    "\n",
    "\n",
    "Tidiness: \n",
    "1. doggo, floofer, pupper, puppo can be combined to one column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweet_predictions Data Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display tweet_predictions dataframe for visual analysis\n",
    "tweet_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displaying additional information on tweet_predictions\n",
    "tweet_predictions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. Our total count for **tweet_predictions** does not match the count for **twitter_archive**. This is likely due to tweets in the archive that have been deleted. We can't really do anything about this, and the missing data will **not** be handled in my cleaning phase \n",
    "2. `tweet_id` may be better off as an object, so we don't accidentally perform arithmatic on it\n",
    "3. There are non-dog breeds in p* columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `p*` Breed Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display boolean of True / False for 'dog_breed'\n",
    "tweet_predictions.p1_dog.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display boolean of True / False for 'dog_breed'\n",
    "tweet_predictions.p2_dog.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display boolean of True / False for 'dog_breed'\n",
    "tweet_predictions.p3_dog.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: There are 500+ values across each p* column that are **not** actual dog breeds. We should check to see if there are any records where **none** of the 3 predictions are actual breeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create mask to find records where none of the 3 breeds are valid dog breeds\n",
    "breed_mask = (tweet_predictions.p1_dog == False) & (tweet_predictions.p2_dog == False) & (tweet_predictions.p3_dog == False)\n",
    "tweet_predictions[breed_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: 324 total records where none of the p* columns contain a valid dog breed. I spot checked a few of these records and confirmed that these appear to be correct, the image does not contain a dog. Since this isn't wrong, we can just include these with the rest of our variables when we combine tweet_predictions with twitter_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tweet_predictions  notes:\n",
    "\n",
    "Quality: \n",
    "1. `p1`, `p2`, `p3` columns have mixed case strings\n",
    "2. `p1`, `p2`, `p3` values have `_` seperated values. May be better to remove\n",
    "\n",
    "\n",
    "Tidiness: \n",
    "1. p* columns can be combined to show the 'best guess' breed, and combined with twitter_archive frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweet_metrics Data Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tweet_metrics dataframe for visual analysis\n",
    "tweet_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display additional information about metrics\n",
    "tweet_metrics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the difference in archive tweets and tweet metric values\n",
    "print(len(twitter_archive.tweet_id) - len(tweet_metrics.tweet_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 29 fewer tweets in tweet_metrics than there are in twitter_archive. This is likely due to tweets from the archive that have been deleted. There isn't any need to drop these fields, they can just exist with null values for tweet_metrics. Best practice would be to combine this dataframe with twitter_archive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the final list of quality/tidiness issues that I will be cleaning. Any prior exploration or notation for issues **not** listed below will **not** be cleaned. There are many more opportunities to assess and clean beyond my final list, but per project specs this is not required beyond (8) Quality issues and (2) Tidiness issues\n",
    "\n",
    "### Quality issues\n",
    "1. **twitter_archive** - 181 retweets\n",
    "\n",
    "2. **twitter_archive** - 78 reply tweets\n",
    "\n",
    "3. **twitter_archive** - Non-names in `name` column\n",
    "\n",
    "4. **twitter_archive** - Records with null value in`expanded_urls` \n",
    "\n",
    "5. **twitter_archive** - Unnecessary href tags in `source`\n",
    "\n",
    "7. **twitter_archive** - `rating_denominator` + `rating_numerator` correct valid ratings manually\n",
    "\n",
    "8. **twitter_archive** - `rating_denominator` records with multiple dogs\n",
    "\n",
    "8. **twitter_archive** - `rating_numerator` records >= 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Tidiness issues\n",
    "1. **twitter_archive** - Dog type (ie: doggo, floofer, pupper, puppo) should only be 1 column\n",
    "2. **tweet_predictions** - p* columns can be combined to show the breed with highest confidence, and combined with **twitter_archive** frame\n",
    "3. **tweet_metrics** - `retweet_count` and `favorite_count` can be combined to **twitter_archive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 32,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make copies of original pieces of data\n",
    "twitter_archive_clean = twitter_archive.copy()\n",
    "tweet_predictions_clean = tweet_predictions.copy()\n",
    "tweet_metrics_clean = tweet_metrics.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue #1: 181 retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: Per project specs, we should only have original tweets in our final dataframe. The columns `retweeted_status_id`, `retweeted_status_user_id`, and `retweeted_status_timestamp` have a total 181 records that are non-null. This indicates retweets, or non-original tweets, in our dataframe. I am going to drop every record that has a non-null value in these columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verifying the correct number of retweets in the archive\n",
    "print(len(twitter_archive_clean.retweeted_status_id.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new copy of the clean dataframe with only null retweet values\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean.retweeted_status_id.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping freshly empty columns related to retweets\n",
    "## Resetting the index to update after we dropped the retweets \n",
    "column_list = ['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp']\n",
    "twitter_archive_clean.drop(columns =column_list, inplace = True)\n",
    "twitter_archive_clean.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Subtracting our new list len from the original len to confirm dropped row #\n",
    "print(len(twitter_archive.tweet_id) - len(twitter_archive_clean.tweet_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the new columns and index updates \n",
    "## I know that I don't have to reset the index until I'm done dropping rows\n",
    "### This just helps everything feel more organized \n",
    "twitter_archive_clean.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue #2: 78 reply tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### Define: Per project specs, we should only have original tweets in our final dataframe. The columns `in_reply_to_status_id` and `in_reply_to_user_id`,  have a total 78 records that are non-null. This indicates replies to tweets, or non-original tweets, in our dataframe. I am going to drop every record that has a non-null value in these columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verifying the correct number of replies in the archive\n",
    "print(len(twitter_archive_clean.in_reply_to_status_id.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Code above is stating 77 replies. This is 1 off from our original count. We may have dropped a tweet in our last clean step that was both a retweet *and* a reply. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new copy of our clean dataframe with only null reply values\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean.in_reply_to_status_id.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping freshly empty columns related to retweets\n",
    "## Resetting the index to update after we dropped the retweets \n",
    "column_list = ['in_reply_to_status_id', 'in_reply_to_user_id']\n",
    "twitter_archive_clean.drop(columns =column_list, inplace = True)\n",
    "twitter_archive_clean.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtracting our new list len from the original len to confirm dropped row #\n",
    "print(len(twitter_archive.tweet_id) - len(twitter_archive_clean.tweet_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the new columns and index updates \n",
    "## I know that I don't have to reset the index until I'm done dropping rows\n",
    "### This just helps everything feel more organized \n",
    "twitter_archive_clean.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue #3: Non-names in `name` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: In the data assessment phase, I confirmed that there are invalid names in the `name` column. Additionally, null values in `name` are represented as 'None'. Both of these issues will be cleaned in this step. Numpys .nan function will be used to change these 'bad' values to null. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mask with regex to identify names that start with a lower case letter\n",
    "name_mask = (twitter_archive_clean.name.str.contains('^[a-z]', regex = True)) | (twitter_archive_clean.name.str.contains('None'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .loc to go through the values in the name column \n",
    "## Replacing any invalid names caught by name_mask with null, using numpy .nan function\n",
    "twitter_archive_clean.loc[name_mask, 'name'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see any records with lowercase strings in 'name', or 'None'\n",
    "twitter_archive_clean[name_mask].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue #4: Null values in `expanded_urls`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: In the assessment section, I identified 3 tweets that were missing `expanded_urls`, that were **not** replies or retweets. I checked each tweet manually and validated that these were not actual ratings, nor did they include any dogs. I feel the best solution for these 3 tweets is to remove them from our final data set. During clean, I will drop them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new copy of twitter_archive_clean that does not contain null url values\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean.expanded_urls.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking expanded_urls to see if any are still null\n",
    "twitter_archive_clean.expanded_urls.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue #5: href tags in `source`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: The `source` column contains unecessary href tags. Meaning, the text string contained within the href tags is legitimate, however there is no use for the tag itself. This can be simplified by extracting the actual value from these records and scrubbing the href tag. Using regex and .str.extract to grab the text string from inside the href tags.\n",
    "\n",
    "There are only 4 values in this column:\n",
    "\n",
    "1. Twitter for iPhone\n",
    "2. Vine - Make a Scene\n",
    "3. Twitter Web Client\n",
    "4. TweetDeck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting a list of current source values with href tags\n",
    "twitter_archive_clean.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://www.rexegg.com/regex-quickstart.html\n",
    "## Using regex to extract the text value from inside the url tags\n",
    "twitter_archive_clean.source = twitter_archive_clean.source.str.extract('^<a.+>(.+)</a>$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing my source list again to confirm the str extract worked \n",
    "twitter_archive_clean.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue #6: `rating_denominator` + `rating_numerator` correct valid ratings manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: In the assessment phase, I discovered several tweets that did not have a demoninator of 10. Based on my visual analysis, I concluded that these ratings were not valid, and were caused by either: \n",
    "1. The script that originally grabbed the tweet archive data identified an incorrect rating in `text`. The correct rating exists in `text` as well. \n",
    "2. The rating is for a group of dogs\n",
    "\n",
    "This step will be to clean issue #1. I will manually update the ratings for any applicable tweets below. If there is no valid rating in `text`, then the tweet will be dropped in the next clean step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisiting the list of != denominators in our clean data frame\n",
    "column_list = ['tweet_id', 'text', 'rating_numerator', 'rating_denominator' ]\n",
    "denom_mask = (twitter_archive_clean.rating_denominator != 10)\n",
    "twitter_archive_clean[denom_mask][column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any potential decminal values\n",
    "twitter_archive_clean[twitter_archive_clean.text.str.contains(r\"(\\d+\\.\\d*\\/\\d+)\")][['tweet_id','text', 'rating_numerator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "1. tweet_id = 810984652412424192 : \n",
    "No valid rating\n",
    "2. tweet_id = 740373189193256964 : 14/10\n",
    "3. tweet_id = 716439118184652801 : 11/10\n",
    "4. tweet_id = 682962037429899265 : 10/10\n",
    "5. tweet_id = 666287406224695296 : 9/10\n",
    "6. tweet_id = 883482846933004288 : 14/10\n",
    "7. tweet_id = 786709082849828864 : 10/10\n",
    "8. tweet_id = 778027034220126208 : 12/10\n",
    "\n",
    "All other tweets in this list contain multiple dogs. There are 4 tweets able to be updated manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually updating tweet_id with the corrected values in `rating_numerator` and `rating_denominator`\n",
    "rating_mask = twitter_archive_clean.tweet_id == 740373189193256964\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_numerator'] = 14\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_denominator'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually updating tweet_id with the corrected values in `rating_numerator` and `rating_denominator`\n",
    "rating_mask = twitter_archive_clean.tweet_id == 716439118184652801\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_numerator'] = 11\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_denominator'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually updating tweet_id with the corrected values in `rating_numerator` and `rating_denominator`\n",
    "rating_mask = twitter_archive_clean.tweet_id == 682962037429899265\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_numerator'] = 10\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_denominator'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually updating tweet_id with the corrected values in `rating_numerator` and `rating_denominator`\n",
    "rating_mask = twitter_archive_clean.tweet_id == 666287406224695296\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_numerator'] = 9\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_denominator'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually updating tweet_id with the corrected values in `rating_numerator` and `rating_denominator`\n",
    "rating_mask = twitter_archive_clean.tweet_id == 883482846933004288\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_numerator'] = 14\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_denominator'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually updating tweet_id with the corrected values in `rating_numerator` and `rating_denominator`\n",
    "rating_mask = twitter_archive_clean.tweet_id == 786709082849828864\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_numerator'] = 10\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_denominator'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually updating tweet_id with the corrected values in `rating_numerator` and `rating_denominator`\n",
    "rating_mask = twitter_archive_clean.tweet_id == 778027034220126208\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_numerator'] = 12\n",
    "twitter_archive_clean.loc[rating_mask, 'rating_denominator'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean[(twitter_archive_clean.tweet_id == 740373189193256964) | \\\n",
    "                     (twitter_archive_clean.tweet_id == 716439118184652801) | \\\n",
    "                     (twitter_archive_clean.tweet_id == 682962037429899265) | \\\n",
    "                     (twitter_archive_clean.tweet_id == 666287406224695296) | \\\n",
    "                     (twitter_archive_clean.tweet_id == 883482846933004288) | \\\n",
    "                     (twitter_archive_clean.tweet_id == 786709082849828864) | \\\n",
    "                     (twitter_archive_clean.tweet_id == 778027034220126208) ][column_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue #7: `rating_denominator` records with multiple dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: In the assessment phase, I discovered several tweets that did not have a demoninator of 10. Based on my visual analysis, I concluded that these ratings were not valid, and were caused by either: \n",
    "1. The script that originally grabbed the tweet archive data identified an incorrect rating in `text`. The correct rating exists in `text` as well. \n",
    "2. The rating is for a group of dogs\n",
    "\n",
    "This step will be to clean issue #2. Any tweets that still exist with a `rating_denominator` != 10 are either multiples, or do not have a valid rating in the tweet `text`. These will be dropped as they are not helpful for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the remaining tweets to drop\n",
    "twitter_archive_clean[denom_mask][column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new copy of the data frame with only records that have rating_denominator == 10\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean.rating_denominator == 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if any != 10 still exist\n",
    "twitter_archive_clean[denom_mask][column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double checking by displaying the value counts for rating_denominator\n",
    "twitter_archive_clean.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issue #8: `rating_numerator` records >= 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: In the assessment phase, I discovered several tweets that had a numerator rating >= 15. Upon further inspection, I confirmed that these tweets were either not valid (ie: not ratings), or they were nonsensical (even by WeRateDogs standards). I will drop these tweets in this step, as they are not useful for analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the tweets with rating_numerator >= 15\n",
    "numerator_mask = (twitter_archive_clean.rating_numerator >= 15)\n",
    "twitter_archive_clean[numerator_mask][column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new copy of twitter_archive_clean with only numerators < 15\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean.rating_numerator < 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if any < 15 values still exist\n",
    "twitter_archive_clean[numerator_mask][column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double checking by displaying the value counts\n",
    "twitter_archive_clean.rating_numerator.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness Issue #1: Combine dog type (ie: `doggo`, `floofer`, `pupper`, `puppo`) into `dog_type` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: In the twitter_archive_clean dataframe, there are 4 columns representing a \"type\" of dog. This does not adhere to the rules of tidy data. This clean step will be to combine the following columns to one: \n",
    "1. `doggo`\n",
    "2. `floofer` \n",
    "3. `pupper`\n",
    "4. `puppo`\n",
    "\n",
    "Additionally, instead of NaN appearing for null values, these columns contain text string 'None'. I will also be replacing these values with null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displaying DF for visual review\n",
    "twitter_archive_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://stackoverflow.com/questions/39291499/how-to-concatenate-multiple-column-values-into-a-single-column-in-pandas-datafra\n",
    "## Using .apply to join the dog type columns into singular column dog_type\n",
    "column_list = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "twitter_archive_clean['dog_type'] = twitter_archive_clean[column_list].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying updated DF for review\n",
    "twitter_archive_clean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the dog columns and resetting the index\n",
    "twitter_archive_clean.drop(columns =column_list, inplace = True)\n",
    "twitter_archive_clean.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displaying a list of values in dog_type\n",
    "twitter_archive_clean.dog_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: Looks like we have 11 tweets with more than one `dog_type`. Since we combined the columns uniformly, this is ok. I would like to go back through these tweets and remove any of the 'None' values. They are unnecessary (as they are null), and make the column look a little clunky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing None_None_None_None with null, as these tweets contain no valid dog_type\n",
    "twitter_archive_clean.loc[twitter_archive_clean.dog_type.str.contains('None_None_None_None'), 'dog_type'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the str to strip out erroneous 'none' values\n",
    "twitter_archive_clean.dog_type = twitter_archive_clean.dog_type.str.replace('None_None_pupper_None', 'pupper')\n",
    "twitter_archive_clean.dog_type = twitter_archive_clean.dog_type.str.replace('doggo_None_None_None', 'doggo')\n",
    "twitter_archive_clean.dog_type = twitter_archive_clean.dog_type.str.replace('None_None_None_puppo', 'puppo')\n",
    "twitter_archive_clean.dog_type = twitter_archive_clean.dog_type.str.replace('doggo_None_pupper_None', 'doggo_pupper')\n",
    "twitter_archive_clean.dog_type = twitter_archive_clean.dog_type.str.replace('None_floofer_None_None', 'floofer')\n",
    "twitter_archive_clean.dog_type = twitter_archive_clean.dog_type.str.replace('doggo_None_None_puppo', 'doggo_puppo')\n",
    "twitter_archive_clean.dog_type = twitter_archive_clean.dog_type.str.replace('doggo_floofer_None_None', 'doggo_floofer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notes: I tried to accomplish the above using .str.exract, but I couldn't figure out the regex syntax*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revewing the new dog_type values\n",
    "twitter_archive_clean.dog_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking table to ensure columns dropped ok\n",
    "twitter_archive_clean.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness Issue #2: Combine p* columns in tweet_predictions_clean into twitter_archive_clean as `breed`tweet_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: - p* columns can be combined to show the breed with highest confidence, and added to twitter_archive_clean as `breed`. Ideally, I would like to take the prediction with the highest confidence and assign that to `breed`. Then, take the `breed` column and join it to twitter_archive_clean. <br>\n",
    "Per the project specs, the `p1` column contains the breed with the highest confidence. Meaning, `p1_conf` is also the highest confidence level for each image. I am going to ignore values where dog != True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_predictions_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://www.geeksforgeeks.org/numpy-select-function-python/\n",
    "## Creating ordered lists for numpy select function \n",
    "conditions = [(tweet_predictions_clean.p1_dog == True),(tweet_predictions_clean.p2_dog == True),(tweet_predictions_clean.p3_dog == True)]\n",
    "breed_list = [tweet_predictions_clean.p1, tweet_predictions_clean.p2, tweet_predictions_clean.p3]\n",
    "conf_list = [tweet_predictions_clean.p1_conf,tweet_predictions_clean.p2_conf,tweet_predictions_clean.p3_conf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://www.geeksforgeeks.org/numpy-select-function-python/\n",
    "## Using numpy select to grab values for new columns based on whether or not the breed is a dog\n",
    "tweet_predictions_clean['breed'] = np.select(conditions, breed_list, default = np.nan)\n",
    "tweet_predictions_clean['confidence'] = np.select(conditions, conf_list, default = np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that numpy select worked, and we now have both columns populated\n",
    "column_list = ['tweet_id', 'breed', 'confidence']\n",
    "tweet_predictions_clean[column_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notes: Now that we have successfully created new columns to show the best possible breed prediction and confidence level, these columns need to be joined on the main dataframe, twitter_archive_clean, in order to maintain best tidiness standards of having a master dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://datacarpentry.org/python-socialsci/11-joins/index.html#:~:text=We%20can%20join%20columns%20from,want%20using%20the%20how%20parameter.\n",
    "## Using pandas merge to join the frames based on tweet_id\n",
    "twitter_archive_clean = pd.merge(twitter_archive_clean, tweet_predictions_clean[column_list], how='left', on='tweet_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display twitter_archive_clean to confirm merge was successful\n",
    "twitter_archive_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking value counts\n",
    "twitter_archive_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness Issue #3: Combine tweet_metrics_clean `retweet_count` and `favorite_count` to twitter_archive_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: `retweet_count` and `favorite_count` in tweet_metrics_clean can be joined with twitter_archive_clean. Best practices indicate that it is better to have as few dataframes as possible. Anything able to be merged, should be. Since there are only 2 (non-id) columns in this frame, it is better to join on our master frame, twitter_archive_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tweet_metrics_clean for visual review\n",
    "tweet_metrics_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://datacarpentry.org/python-socialsci/11-joins/index.html#:~:text=We%20can%20join%20columns%20from,want%20using%20the%20how%20parameter.\n",
    "## Using pandas merge to join frames on tweet_id\n",
    "twitter_archive_clean = pd.merge(twitter_archive_clean, tweet_metrics_clean, how='left', on='tweet_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm merge was successful \n",
    "twitter_archive_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking value counts\n",
    "twitter_archive_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store master data frame to .csv\n",
    "twitter_archive_clean.to_csv(\"twitter_archive_master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading our master file into a new frame 'df' for analysis\n",
    "df = pd.read_csv(\"twitter_archive_master.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the most popular dog breed on WeRateDogs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: The first insight I will be looking to gain, is a better understanding of which dog breeds tend to be the most popular on WeRateDogs. Since the breed list in our data set is dervived from a prediction list, we cannot assume that these insights will be 100% accurate. We also have many tweets that do not have a breed listed at all. However, the predictions were made via a neural network using a sample of 100 tweets from our dataset. The insights gleaned from analyzing breed information will likely still be meaningful <br>\n",
    "There are 2 ideas to explore in this section. \n",
    "1. Which dog breed appears most frequently in the tweets\n",
    "2. Which dog breed is the most favorited "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distinct breed count\n",
    "print('There are {} distinct breeds'.format(len(df.breed.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ref: https://stackabuse.com/seaborn-bar-plot-tutorial-and-examples/\n",
    "## Create bar plot of the top 5 most popular breeds, by tweet frequency\n",
    "pop_breeds = df.breed.value_counts().head(5)\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "\n",
    "pop_bar = sns.barplot(x=pop_breeds, y=pop_breeds.index, palette='hls')\n",
    "pop_bar.set_title('WeRateDogs Top 5 Breed Listing')\n",
    "pop_bar.set_xlabel('Tweet Frequency')\n",
    "pop_bar.set_ylabel('Dog Breed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The top 5 dog breeds comprise {} % of total archived tweets'.format(round(sum(pop_breeds) * 100 / len(df.tweet_id))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the first visualization, it is apparent that Golden Retrievers are the most popular dog breed tweeted about by far. Followed by Labrador Retriever, Pembroke, Chihuahua, and finally Pug in fifth place. Again, this is based on a neural networks **prediction** of dog breed. This could mean that Golden Retriever owners are generally more active, or that a large number of WeRateDog followers own Golden Retrievers. There's no further insight to be gleaned from this, we can't determine the actual reason without additional studies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating variables fav_breeds and fav_breeds_sorted to group breed by favorites count\n",
    "fav_breeds = df.groupby('breed')\n",
    "fav_breeds = fav_breeds['favorite_count'].sum()\n",
    "fav_breeds_sorted = fav_breeds.sort_values(ascending=False)\n",
    "fav_breeds_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The top 5 dog breeds comprise {}% of total favorite tweets'.format(round(sum(fav_breeds_sorted.head(5)) * 100 / sum(fav_breeds_sorted))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I group our favorite count by the breed association, I can see that again, Golden Retrievers are the most popular dog breed. At the time of this report there are 1,693,324 favorites on tweets with Golden Retrievers. The second most popular again is Labrador Retriever, third is Pembroke, and fourth is Chihuahua. This matches our previous analysis of most frequently occurring breeds. Fifth place is French Bulldog, which differs from our frequency count, where Pug lands in the fifth spot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are users making the most popular tweets from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: I would also like to know where users are tweeting from. I'd like to gain insight into not only the overall source popularity, but if there is any potential correlation in the popularity of the tweet (retweets + favorites) and which platform they were tweeted from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total number of tweets for each source\n",
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group retweet and favorite count by platform\n",
    "platform_group = df.groupby('source')[['retweet_count','favorite_count']].sum()\n",
    "platform_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display percentage of tweets made from the top platform\n",
    "print('Of every tweet in our archive, {}% were made from the top platform'.format(round(sum(df.source.value_counts().head(1)) * 100 / sum(df.source.value_counts()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ref: https://seaborn.pydata.org/generated/seaborn.scatterplot.html\n",
    "## Create a graph to explore the relationship between favorite / retweet count and platform\n",
    "plt.figure(figsize = (10,5))\n",
    "\n",
    "pop_platform = sns.scatterplot(x=df['retweet_count'], y=df['favorite_count'], hue=df['source'], palette='hls')\n",
    "pop_platform.set_xlabel('Number of Retweets')\n",
    "pop_platform.set_ylabel('Number of Favorites')\n",
    "pop_platform.set_title('Tweet Popularity by Platform');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the scatterplot above as a table\n",
    "column_list = ['tweet_id','source','retweet_count','favorite_count']\n",
    "df[column_list].sort_values(by=['retweet_count', 'favorite_count'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the analysis above, it becomes clear that the most popular platform to tweet from is **Twitter for iPhone**, then **Vine - Make a Scene**, **Twitter Web Client**, and finally **TweetDeck**. This also happens to be the order for our most popular tweets. The most popular tweets (by favorite_count and retweet_count), all come from **Twitter for iPhone**. \n",
    "\n",
    "It appears that overall, the most popular tweets are generated on the most popular platform. Correlation does not imply causation, there is no immediate reason to assume that tweets become popular *because* they were created on an iPhone. However, it is interesting to note. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the most popular dog names by frequency?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: I am also looking to gain insight into the popularity of certain dog names. Are there dog names that appear much more often than others? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display percentage of popular names / total name count\n",
    "name_counts = df['name'].value_counts().sum()\n",
    "top_five = df['name'].value_counts().head(5).sum()\n",
    "\n",
    "print('The most popular dog names account for {}% of total names'.format(round(top_five * 100/ name_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of occurances for most popular dog name\n",
    "print('The most popular dog name has {} occurrances'.format(df.name.value_counts().head(1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the list of 5 most popular dog names\n",
    "print('The top 5 most popular dog names are:')\n",
    "df['name'].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis above, it does appear that there are a few names that occur more frequently than others. We have a large number of tweets that do not include a name, so my analysis was only based off of tweets that did include names. The most popular dog names by frequency are: **Lucy**, **Charlie**, **Oliver**, **Cooper**, and **Penny**. Even though these names appeared the most frequently, I am not sure if it is statistically significant. The top 5 names only comprised 4% of the total tweet list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "1. **Golden Retrievers** are the overall most popular dog, both in frequency of tweets, and in favorited tweets. \n",
    "2. The most popular tweets are created on **Twitter for iPhone**, which also happens to be where the most tweets are sent from\n",
    "3. The most popular names do not appear to be statistically significant when compared against the total number of tweets"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
